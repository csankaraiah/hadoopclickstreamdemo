Business Case 1

##### Show the source tables 

/***** GMI source tables on MS SQL DB******/
## Use DBVizualizer to show all tables


select *  FROM INFORMATION_SCHEMA.Tables;

select * from products;

select * from users;


/***** GMI source tables on MySQL DB******/
## Use DBVizualizer to show all tables

use test;

select * from Products_URL;

select * from User_WebID;




/******** Use scoop to import these tables ******/
## Login to HDP cluster 
ssh root@172.16.149.151
hadoop


#########  Load data from MS SQL server #################

Download the jar file for MS SQL server
http://www.microsoft.com/en-us/download/confirmation.aspx?id=11774

Copy that jar file to the sandbox

scp sqljdbc_4.0.2206.100_enu.tar.gz root@172.16.149.153:~/

Login to sandbox

cd ~
gunzip sqljdbc_4.0.2206.100_enu.tar.gz

tar -xvf sqljdbc_4.0.2206.100_enu.tar

cp /root/sqljdbc_4.0/enu/sqljdbc4.jar /usr/hdp/2.3.0.0-2557/sqoop/lib/


sqoop import \
	--connect "jdbc:sqlserver://ko3pkhm1fd.database.windows.net:1433;database=GMIdb;username=chakra;password=Ch@11cha" \
	--table products \
	--hive-import \
	-- --schema dbo 
	
sqoop import \
	--connect "jdbc:sqlserver://ko3pkhm1fd.database.windows.net:1433;database=GMIdb;username=chakra;password=Ch@11cha" \
	--table consumers \
	--hive-import \
	-- --schema dbo 




####### Load data from MySQL DB ##############

sqoop import \
	--connect jdbc:mysql://gmihdp.cloudapp.net:3306/test \
	--username trucker1 \
	--password trucker \
	--table Products_URL \
	-m 1 \
	--target-dir demo \
	--hive-import 



sqoop import \
	--connect jdbc:mysql://gmihdp.cloudapp.net:3306/test \
	--username trucker1 \
	--password trucker \
	--table User_WebID \
	-m 1 \
	--target-dir demo \
	--hive-import




/******* Import user session logs *******/

File is provided by the IT team and you use webHDFS to import that data

Use file ominiture.0.tsv

Import file Ominilog using webHDFS & HCatalog.

Create a table using that file names omniturelog



/******** Perform transformation on the data sets *******/


CREATE VIEW omniture AS 
SELECT col_2 ts, col_8 ip, col_13 url, col_14 swid, col_50 city, col_51 country, col_53 state from omniturelog;



create table GMI_webloganalytics as 
select to_date(o.ts) logdate, 
o.url, o.ip, o.city, 
upper(o.state) state, 
o.country, p.productid, 
CAST(datediff( from_unixtime( unix_timestamp() ), 
from_unixtime( unix_timestamp(u.birth_dt, 'dd-MMM-yy'))) / 365  AS INT) age, u.gender_flg gender 
from omniture o inner join Products_URL p on o.url = p.url_id 
    left outer join User_WebID u on o.swid = concat('{', u.swid , '}')

Open this table in Tableau to do further analysis


##############################################################################

Business Case 2


/********* Import weather data to show schema on read **********/

Import file Weather data using WebHDFS using Ambari
1. Create a user in Ambari with username hive and give it admin previleges
2. Login as hive user and create a new directory under /user/hive
named weather_data.
3. Upload GMI_Weather_data to that directory 


Create schema on read table 

/****** Weather data for schema on read *******/

drop table weather_gmi_dy

create external table weather_gmi_dy(variables array<string>)
ROW FORMAT DELIMITED
    COLLECTION ITEMS TERMINATED BY ','
LOCATION '/user/hive/weather_data';


select variables[1] from  weather_gmi_dy;



/****** Sample2 for schema on read *******/

create external table if not exists weather_gmi_dy1 
( yearmonth bigint,
  episode_id bigint,
  event_id bigint,
  location_index bigint,
  range1 double,
  azimuth string,
  location string,
  latitude double,
  longitude double,
  lat2 bigint,
  lon2 bigint) 
  row format delimited fields terminated by ',' 
  location '/user/hive/weather_data';

select* from weather_gmi_dy1;


##############################################################################

Business Case 3

/****** Stream Twitter data using Storm ************/
Login to Intellij

And show the sample code and run Storm topology and also login a text from twitter account. 

have a sample file already loaded to HDFS 

drop table gmi_tweet;

CREATE TABLE gmi_tweet (
  tweet_spout string,
  tweet_stream string,
  tweet_userid string,
  tweet_txt1 string,
  tweet_txt2 string,
  tweet_txt3 string,
  tweet_txt4 string)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ',';


LOAD DATA INPATH '/user/hive/gmi_tweet/tweets_GMI_0810.txt' INTO TABLE gmi_tweet;

create table gmi_tweet_count as 
select cwt_tweet_word, count(*) as tweet_cnt from gmi_travel_tweet_analysis 
group by cwt_tweet_word;

ALTER TABLE gmi_travel_tweet_analysis RENAME TO gmi_tweet_analysis


create table gmi_tweet_analysis as 
SELECT gmi_tweet_word
FROM gmi_tweet LATERAL VIEW explode(split(tweet_txt1, ' ')) wd AS gmi_tweet_word 
where lower(gmi_tweet_word) not in ('a',
'about',
'above',
'across',
'after',
'afterwards',
'again',
'against',
'all',
'almost',
'alone',
'along',
'already',
'also',
'although',
'always',
'am',
'among',
'amongst',
'amoungst',
'amount',
'an',
'and',
'another',
'any',
'anyhow',
'anyone',
'anything',
'anyway',
'anywhere',
'are',
'around',
'as',
'at',
'back',
'be',
'became',
'because',
'become',
'becomes',
'becoming',
'been',
'before',
'beforehand',
'behind',
'being',
'below',
'beside',
'besides',
'between',
'beyond',
'bill',
'both',
'bottom',
'but',
'by',
'call',
'can',
'cannot',
'cant',
'co',
'computer',
'con',
'could',
'couldnt',
'cry',
'de',
'describe',
'detail',
'do',
'done',
'down',
'due',
'during',
'each',
'eg',
'eight',
'either',
'eleven',
'else',
'elsewhere',
'empty',
'enough',
'etc',
'even',
'ever',
'every',
'everyone',
'everything',
'everywhere',
'except',
'few',
'fifteen',
'fify',
'fill',
'find',
'fire',
'first',
'five',
'for',
'former',
'formerly',
'forty',
'found',
'four',
'from',
'front',
'full',
'further',
'get',
'give',
'go',
'had',
'has',
'hasnt',
'have',
'he',
'hence',
'her',
'here',
'hereafter',
'hereby',
'herein',
'hereupon',
'hers',
'herse"',
'him',
'himse"',
'his',
'how',
'however',
'hundred',
'i',
'ie',
'if',
'in',
'inc',
'indeed',
'interest',
'into',
'is',
'it',
'its',
'itse"',
'keep',
'last',
'latter',
'latterly',
'least',
'less',
'ltd',
'made',
'many',
'may',
'me',
'meanwhile',
'might',
'mill',
'mine',
'more',
'moreover',
'most',
'mostly',
'move',
'much',
'must',
'my',
'myse"',
'name',
'namely',
'neither',
'never',
'nevertheless',
'next',
'nine',
'no',
'nobody',
'none',
'noone',
'nor',
'not',
'nothing',
'now',
'nowhere',
'of',
'off',
'often',
'on',
'once',
'one',
'only',
'onto',
'or',
'other',
'others',
'otherwise',
'our',
'ours',
'ourselves',
'out',
'over',
'own',
'part',
'per',
'perhaps',
'please',
'put',
'rather',
're',
'same',
'see',
'seem',
'seemed',
'seeming',
'seems',
'serious',
'several',
'she',
'should',
'show',
'side',
'since',
'sincere',
'six',
'sixty',
'so',
'some',
'somehow',
'someone',
'something',
'sometime',
'sometimes',
'somewhere',
'still',
'such',
'system',
'take',
'ten',
'than',
'that',
'the',
'their',
'them',
'themselves',
'then',
'thence',
'there',
'thereafter',
'thereby',
'therefore',
'therein',
'thereupon',
'these',
'they',
'thick',
'thin',
'third',
'this',
'those',
'though',
'three',
'through',
'throughout',
'thru',
'thus',
'to',
'together',
'too',
'top',
'toward',
'towards',
'twelve',
'twenty',
'two',
'un',
'under',
'until',
'up',
'upon',
'us',
'very',
'via',
'was',
'we',
'well',
'were',
'what',
'whatever',
'when',
'whence',
'whenever',
'where',
'whereafter',
'whereas',
'whereby',
'wherein',
'whereupon',
'wherever',
'whether',
'which',
'while',
'whither',
'who',
'whoever',
'whole',
'whom',
'whose',
'why',
'will',
'with',
'within',
'without',
'would',
'yet',
'you',
'your',
'yours',
'yourself',
'yourselves');




##### HDP cleanup

drop table product_url ;

hdfs dfs -rm -r /user/hdfs/product_url


hdfs dfs -rm -r /user/root/product_url


Removed ominilog file from /user/hive
Removed weahter data file under /user/hive/weather_data

drop table cwt_tweet;

drop table omniturelog;

drop table ominilog;

drop view omniture;

drop table CWT_webloganalytics;

drop table weather_cwt_dy;

drop table weather_cwt_dy1 ;

drop table cwt_tweet_count;

drop table cwt_travel_tweet_analysis;

create table cwt_tweet_count as 
select cwt_tweet_word, count(*) as tweet_cnt from cwt_travel_tweet_analysis 
group by cwt_tweet_word;


create table cwt_travel_tweet_analysis as 



